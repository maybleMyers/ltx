"""
LTX-2 Video Generation UI (lt1.py)

Gradio-based web interface for LTX-2 video generation with:
- Two-stage pipeline (low-res + hi-res refinement)
- Joint audio-video generation
- Image conditioning (I2V)
- LoRA support
- Memory optimization (offload, FP8, block swap)
- Prompt enhancement with Gemma
"""

import os
import sys

# Try to use local patched gradio from modules/ if available (allows jobs to continue after browser disconnect)
_modules_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "modules")
if os.path.exists(_modules_path):
    sys.path.insert(0, _modules_path)

import gradio as gr
from gradio import themes
from gradio.themes.utils import colors
import time
import random
import subprocess
import re
from typing import Generator, List, Tuple, Optional, Dict
import threading
import json
from PIL import Image

# Global state
stop_event = threading.Event()
current_process = None
current_output_filename = None

# Defaults configuration
UI_CONFIGS_DIR = "ui_configs"
LT1_DEFAULTS_FILE = os.path.join(UI_CONFIGS_DIR, "lt1_defaults.json")


# =============================================================================
# Progress Parsing
# =============================================================================

def parse_ltx_progress_line(line: str) -> Optional[str]:
    """Parse LTX-2 generation progress output."""
    line = line.strip()

    # LTX-specific progress messages
    if ">>> Loading text encoder" in line:
        return "Loading text encoder..."
    if ">>> Enhancing prompt" in line:
        return "Enhancing prompt with Gemma..."
    if ">>> Encoding prompts" in line:
        return "Encoding text prompts..."
    # One-stage pipeline
    if ">>> One-stage: Loading" in line:
        return "One-stage: Loading models..."
    if ">>> One-stage: Generating" in line:
        return "One-stage: Generating at full resolution..."
    if ">>> One-stage completed" in line:
        match = re.search(r'(\d+\.?\d*)s', line)
        if match:
            return f"One-stage completed ({match.group(1)}s)"
    # Refine-only pipeline
    if ">>> Refine-only mode" in line:
        return "Refine-only: Loading input video..."
    if ">>> Input video encoded" in line:
        match = re.search(r'(\d+\.?\d*)s', line)
        if match:
            return f"Video encoded ({match.group(1)}s)"
    # Two-stage pipeline
    if ">>> Stage 1: Loading" in line:
        return "Stage 1: Loading models..."
    if ">>> Stage 1: Generating" in line:
        return "Stage 1: Low-res generation..."
    if ">>> Stage 1 completed" in line:
        match = re.search(r'(\d+\.?\d*)s', line)
        if match:
            return f"Stage 1 completed ({match.group(1)}s)"
    if ">>> Upsampling" in line:
        return "Upsampling latents (2x)..."
    if ">>> Stage 2: Loading" in line:
        return "Stage 2: Loading refined model..."
    if ">>> Stage 2: Refining" in line:
        return "Stage 2: Hi-res refinement..."
    if ">>> Stage 2 completed" in line:
        match = re.search(r'(\d+\.?\d*)s', line)
        if match:
            return f"Stage 2 completed ({match.group(1)}s)"
    if ">>> Decoding video" in line:
        return "Decoding video from latents..."
    if ">>> Decoding audio" in line:
        return "Decoding audio..."
    if ">>> Encoding video" in line:
        return "Encoding video to MP4..."
    if ">>> Done!" in line:
        return "Generation complete!"
    if ">>> Total generation time" in line:
        match = re.search(r'(\d+\.?\d*)s', line)
        if match:
            return f"Total time: {match.group(1)}s"

    # TQDM progress bar parsing
    match = re.search(r'(\d+)%\|.*?\|\s*(\d+)/(\d+)\s*\[.*?<([\d:]+)', line)
    if match:
        percent = match.group(1)
        current = match.group(2)
        total = match.group(3)
        eta = match.group(4)
        return f"Denoising: {percent}% ({current}/{total}) ETA: {eta}"

    return None


# =============================================================================
# LoRA Discovery
# =============================================================================

def get_ltx_lora_options(lora_folder: str = "lora") -> List[str]:
    """
    Discover LTX LoRAs in the specified folder.
    Returns ['None'] + list of .safetensors files.
    """
    if not os.path.exists(lora_folder):
        return ["None"]

    lora_items = []
    for item in os.listdir(lora_folder):
        if item.endswith(".safetensors"):
            lora_items.append(item)

    lora_items.sort(key=str.lower)
    return ["None"] + lora_items


def refresh_lora_dropdown(lora_folder: str):
    """Refresh LoRA dropdown choices."""
    new_choices = get_ltx_lora_options(lora_folder)
    return gr.update(choices=new_choices, value="None")


# =============================================================================
# Validation
# =============================================================================

def validate_num_frames(num_frames: int) -> Optional[str]:
    """Validate num_frames is in 8*K + 1 format."""
    if (num_frames - 1) % 8 != 0:
        valid_examples = [8*k + 1 for k in range(1, 20)]
        return f"Error: num_frames must be 8*K + 1 (e.g., {', '.join(map(str, valid_examples[:5]))}...)"
    return None


def validate_resolution(width: int, height: int) -> Optional[str]:
    """Validate resolution is divisible by 64."""
    if width % 64 != 0 or height % 64 != 0:
        return f"Error: width ({width}) and height ({height}) must be divisible by 64"
    return None


def validate_model_paths(checkpoint_path: str, spatial_upsampler_path: str,
                         distilled_lora_path: str, gemma_root: str,
                         is_one_stage: bool = False,
                         is_refine_only: bool = False) -> Optional[str]:
    """Validate all required model paths exist."""
    # Core paths required for all pipelines
    paths = [
        ("LTX Checkpoint", checkpoint_path),
        ("Gemma Root", gemma_root),
    ]
    # Two-stage specific paths (spatial upsampler always needed for two-stage)
    if not is_one_stage and not is_refine_only:
        paths.extend([
            ("Spatial Upsampler", spatial_upsampler_path),
            ("Distilled LoRA", distilled_lora_path),
        ])
    # Refine-only: distilled LoRA is optional, but validate if provided
    if is_refine_only:
        # Only validate distilled LoRA if path is provided
        if distilled_lora_path and distilled_lora_path.strip():
            paths.append(("Distilled LoRA", distilled_lora_path))
    for name, path in paths:
        if not path or not path.strip():
            return f"Error: {name} path is required"
        if not os.path.exists(path):
            return f"Error: {name} not found: {path}"
    return None


# =============================================================================
# Image Dimension Helpers
# =============================================================================

def update_image_dimensions(image_path):
    """Update original dimensions when image is uploaded."""
    if image_path is None:
        return "", gr.update(), gr.update()
    try:
        img = Image.open(image_path)
        w, h = img.size
        original_dims_str = f"{w}x{h}"
        # Calculate dimensions snapped to nearest multiple of 64 while maintaining aspect ratio
        new_w = round(w / 64) * 64
        new_h = round(h / 64) * 64
        new_w = max(64, new_w)
        new_h = max(64, new_h)
        return original_dims_str, gr.update(value=new_w), gr.update(value=new_h)
    except Exception as e:
        print(f"Error reading image dimensions: {e}")
        return "", gr.update(), gr.update()


def update_resolution_from_scale(scale, original_dims):
    """Update dimensions based on scale percentage (divisible by 64)."""
    if not original_dims:
        return gr.update(), gr.update()
    try:
        scale = float(scale) if scale is not None else 100.0
        if scale <= 0:
            scale = 100.0

        orig_w, orig_h = map(int, original_dims.split('x'))
        scale_factor = scale / 100.0

        # Calculate and round to the nearest multiple of 64
        new_w = round((orig_w * scale_factor) / 64) * 64
        new_h = round((orig_h * scale_factor) / 64) * 64

        # Ensure minimum size (must be multiple of 64)
        new_w = max(64, new_w)
        new_h = max(64, new_h)

        return gr.update(value=new_w), gr.update(value=new_h)
    except Exception as e:
        print(f"Error updating from scale: {e}")
        return gr.update(), gr.update()


def calculate_width_from_height(height, original_dims):
    """Calculate width based on height maintaining aspect ratio (divisible by 64)."""
    if not original_dims or height is None:
        return gr.update()
    try:
        height = int(height)
        if height <= 0:
            return gr.update()
        height = (height // 64) * 64
        height = max(64, height)

        orig_w, orig_h = map(int, original_dims.split('x'))
        if orig_h == 0:
            return gr.update()
        aspect_ratio = orig_w / orig_h
        new_width = round((height * aspect_ratio) / 64) * 64
        return gr.update(value=max(64, new_width))

    except Exception as e:
        print(f"Error calculating width: {e}")
        return gr.update()


def calculate_height_from_width(width, original_dims):
    """Calculate height based on width maintaining aspect ratio (divisible by 64)."""
    if not original_dims or width is None:
        return gr.update()
    try:
        width = int(width)
        if width <= 0:
            return gr.update()
        width = (width // 64) * 64
        width = max(64, width)

        orig_w, orig_h = map(int, original_dims.split('x'))
        if orig_w == 0:
            return gr.update()
        aspect_ratio = orig_w / orig_h
        new_height = round((width / aspect_ratio) / 64) * 64
        return gr.update(value=max(64, new_height))

    except Exception as e:
        print(f"Error calculating height: {e}")
        return gr.update()


def get_video_info(video_path: str) -> dict:
    """Get video information using PyAV."""
    try:
        import av
        with av.open(video_path) as container:
            # Get first video stream
            video_stream = container.streams.video[0]

            width = video_stream.width
            height = video_stream.height

            # Calculate FPS from average_rate or guessed_rate
            if video_stream.average_rate:
                fps = float(video_stream.average_rate)
            elif video_stream.guessed_rate:
                fps = float(video_stream.guessed_rate)
            else:
                fps = 30.0  # fallback

            # Calculate duration
            if video_stream.duration and video_stream.time_base:
                duration = float(video_stream.duration * video_stream.time_base)
            elif container.duration:
                duration = container.duration / av.time_base
            else:
                duration = 0

            total_frames = int(video_stream.frames) if video_stream.frames else int(duration * fps)

            return {
                'width': width,
                'height': height,
                'fps': fps,
                'total_frames': total_frames,
                'duration': duration
            }
    except Exception as e:
        print(f"Error extracting video info: {e}")
        return {}


def update_video_dimensions(video_path):
    """Update dimensions and frame count when video is uploaded."""
    if video_path is None:
        return "", gr.update(), gr.update(), gr.update(), gr.update()
    try:
        info = get_video_info(video_path)
        if info:
            w, h = info.get("width", 0), info.get("height", 0)
            if w and h:
                original_dims_str = f"{w}x{h}"
                # Snap to nearest multiple of 64
                new_w = round(w / 64) * 64
                new_h = round(h / 64) * 64
                new_w = max(64, new_w)
                new_h = max(64, new_h)
                # Calculate frame count (snap to 8*K+1 format)
                if info.get("total_frames"):
                    num_frames = info["total_frames"]
                elif info.get("duration") and info.get("fps"):
                    num_frames = int(info["duration"] * info["fps"])
                else:
                    num_frames = 121
                # Snap to nearest 8*K+1
                k = max(1, round((num_frames - 1) / 8))
                num_frames = 8 * k + 1
                fps = info.get("fps", 24)
                return original_dims_str, gr.update(value=new_w), gr.update(value=new_h), gr.update(value=num_frames), gr.update(value=int(fps))
    except Exception as e:
        print(f"Error reading video dimensions: {e}")
    return "", gr.update(), gr.update(), gr.update(), gr.update()


def extract_video_metadata(video_path: str) -> dict:
    """Extract metadata from video file using PyAV."""
    try:
        import av
        with av.open(video_path) as container:
            # Get comment metadata from container
            comment = container.metadata.get('comment', '{}')
            return json.loads(comment)
    except Exception as e:
        print(f"Metadata extraction failed: {str(e)}")
        return {}


def add_metadata_to_video(video_path: str, parameters: dict) -> None:
    """Add generation parameters to video metadata using ffmpeg."""
    # Convert parameters to JSON string
    params_json = json.dumps(parameters, indent=2)

    # Temporary output path
    temp_path = video_path.replace(".mp4", "_temp.mp4")

    # FFmpeg command to add metadata without re-encoding
    cmd = [
        'ffmpeg',
        '-i', video_path,
        '-metadata', f'comment={params_json}',
        '-codec', 'copy',
        temp_path
    ]

    try:
        # Execute FFmpeg command
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        # Replace original file with the metadata-enhanced version
        os.replace(temp_path, video_path)
    except subprocess.CalledProcessError as e:
        print(f"Failed to add metadata: {e.stderr.decode()}")
        if os.path.exists(temp_path):
            os.remove(temp_path)
    except Exception as e:
        print(f"Error: {str(e)}")


def extract_first_frame(video_path: str, output_dir: str = "outputs") -> Optional[str]:
    """Extract the first frame from a video file."""
    try:
        import av
        os.makedirs(output_dir, exist_ok=True)

        with av.open(video_path) as container:
            stream = container.streams.video[0]
            for frame in container.decode(stream):
                # Convert to PIL Image and save
                img = frame.to_image()
                # Save with unique name based on source video
                base_name = os.path.splitext(os.path.basename(video_path))[0]
                output_path = os.path.join(output_dir, f"{base_name}_frame0.png")
                img.save(output_path)
                return output_path
    except Exception as e:
        print(f"Error extracting first frame: {e}")
    return None


def extract_video_details(video_path: str) -> Tuple[dict, str, Optional[str]]:
    """Extract metadata, video information, and first frame."""
    metadata = extract_video_metadata(video_path)
    video_details = get_video_info(video_path)

    # Combine metadata with video details
    for key, value in video_details.items():
        if key not in metadata:
            metadata[key] = value

    # Extract first frame
    first_frame_path = extract_first_frame(video_path)

    # Return metadata, status message, and first frame path
    return metadata, "Video details extracted successfully", first_frame_path


# =============================================================================
# Video Generation
# =============================================================================

def generate_ltx_video(
    # Prompts
    prompt: str,
    negative_prompt: str,
    # Model paths
    checkpoint_path: str,
    distilled_checkpoint: bool,
    stage2_checkpoint: str,
    gemma_root: str,
    spatial_upsampler_path: str,
    distilled_lora_path: str,
    distilled_lora_strength: float,
    # Generation parameters
    mode: str,
    pipeline: str,
    enable_sliding_window: bool,
    width: int,
    height: int,
    num_frames: int,
    frame_rate: float,
    cfg_guidance_scale: float,
    num_inference_steps: int,
    stage2_steps: int,
    seed: int,
    # Image conditioning (for I2V)
    input_image: str,
    image_frame_idx: int,
    image_strength: float,
    # End image conditioning
    end_image: str,
    end_image_strength: float,
    # Anchor image conditioning
    anchor_image: str,
    anchor_interval: int,
    anchor_strength: float,
    anchor_decay: str,
    # Video input (for V2V / refine)
    input_video: str,
    refine_strength: float,
    refine_steps: int,
    # Audio & prompt
    disable_audio: bool,
    enhance_prompt: bool,
    # Memory optimization
    offload: bool,
    enable_fp8: bool,
    enable_dit_block_swap: bool,
    dit_blocks_in_memory: int,
    enable_text_encoder_block_swap: bool,
    text_encoder_blocks_in_memory: int,
    enable_refiner_block_swap: bool,
    refiner_blocks_in_memory: int,
    # LoRA
    lora_folder: str,
    user_lora: str,
    user_lora_strength: float,
    # Output
    save_path: str,
    batch_size: int,
    # Preview Generation
    enable_preview: bool,
    preview_interval: int,
    # Video Continuation (Frame Freezing)
    freeze_frames: int,
    freeze_transition: int,
    # Sliding Window (Long Video)
    sliding_window_size: int,
    sliding_window_overlap: int,
    sliding_window_overlap_noise: float,
    sliding_window_color_correction: float,
) -> Generator[Tuple[List[Tuple[str, str]], Optional[str], str, str], None, None]:
    """
    Generate video using LTX-2 pipeline.

    Yields: (gallery_items, preview_path, status_text, progress_text)
    """
    global stop_event, current_process, current_output_filename
    stop_event.clear()
    current_process = None
    current_output_filename = None

    # Validate inputs
    error = validate_num_frames(int(num_frames))
    if error:
        yield [], None, error, ""
        return

    error = validate_resolution(int(width), int(height))
    if error:
        yield [], None, error, ""
        return

    # Round num_frames to valid 8*K+1 format
    num_frames = int(num_frames)
    k = max(1, round((num_frames - 1) / 8))
    num_frames = 8 * k + 1

    is_one_stage = (pipeline == "one-stage")
    is_refine_only = (pipeline == "refine-only")
    error = validate_model_paths(checkpoint_path, spatial_upsampler_path,
                                  distilled_lora_path, gemma_root,
                                  is_one_stage=is_one_stage,
                                  is_refine_only=is_refine_only)
    if error:
        yield [], None, error, ""
        return

    # Check image for I2V mode
    if mode == "i2v" and not input_image:
        yield [], None, "Error: Input image required for I2V mode", ""
        return

    # Check video for V2V/refine-only mode
    if (mode == "v2v" or pipeline == "refine-only") and not input_video:
        yield [], None, "Error: Input video required for V2V/refine-only mode", ""
        return

    os.makedirs(save_path, exist_ok=True)
    all_generated_videos = []

    for i in range(int(batch_size)):
        if stop_event.is_set():
            current_process = None
            yield all_generated_videos, None, "Generation stopped by user.", ""
            return

        # Seed handling
        current_seed = seed
        if seed == -1:
            current_seed = random.randint(0, 2**32 - 1)
        elif int(batch_size) > 1:
            current_seed = seed + i

        status_text = f"Processing {i+1}/{batch_size} (Seed: {current_seed})"
        yield all_generated_videos.copy(), None, status_text, "Starting generation..."

        timestamp = int(time.time())
        run_id = f"{timestamp}_{random.randint(1000, 9999)}"
        output_filename = os.path.join(save_path, f"ltx_{mode}_{timestamp}_{current_seed}.mp4")
        current_output_filename = output_filename

        # Build command
        command = [
            sys.executable, "ltx_generate_video.py",
            "--checkpoint-path", checkpoint_path,
            "--gemma-root", gemma_root,
            "--prompt", str(prompt),
            "--negative-prompt", str(negative_prompt),
            "--num-frames", str(int(num_frames)),
            "--frame-rate", str(float(frame_rate)),
            "--width", str(int(width)),
            "--height", str(int(height)),
            "--cfg-guidance-scale", str(float(cfg_guidance_scale)),
            "--num-inference-steps", str(int(num_inference_steps)),
            "--stage2-steps", str(int(stage2_steps)),
            "--seed", str(current_seed),
            "--output-path", output_filename,
        ]

        # Pipeline selection
        if is_one_stage:
            command.append("--one-stage")
        elif is_refine_only:
            command.append("--refine-only")
            # Refine-only: distilled LoRA is optional, skip if stage2 checkpoint or distilled checkpoint
            if not distilled_checkpoint and not stage2_checkpoint and distilled_lora_path and distilled_lora_path.strip() and os.path.exists(distilled_lora_path):
                command.extend(["--distilled-lora", distilled_lora_path, str(distilled_lora_strength)])
        else:
            # Two-stage specific: spatial upsampler and distilled LoRA
            command.extend(["--spatial-upsampler-path", spatial_upsampler_path])
            # Skip distilled LoRA if using stage2 checkpoint (full model) or distilled checkpoint
            if not distilled_checkpoint and not stage2_checkpoint:
                command.extend(["--distilled-lora", distilled_lora_path, str(distilled_lora_strength)])

        # Stage 2 checkpoint (full model for stage 2 refinement)
        if stage2_checkpoint and stage2_checkpoint.strip() and os.path.exists(stage2_checkpoint):
            command.extend(["--stage2-checkpoint", stage2_checkpoint])

        # Video input (V2V / refine)
        if input_video:
            command.extend(["--input-video", str(input_video)])
            command.extend(["--refine-strength", str(float(refine_strength))])
            command.extend(["--refine-steps", str(int(refine_steps))])

        # Image conditioning (I2V)
        if mode == "i2v" and input_image:
            command.extend(["--image", str(input_image), str(int(image_frame_idx)), str(float(image_strength))])

        # End image conditioning (place at last frame)
        if end_image:
            last_frame_idx = int(num_frames) - 1
            command.extend(["--image", str(end_image), str(last_frame_idx), str(float(end_image_strength))])

        # Anchor image conditioning (periodic guidance)
        if anchor_interval and int(anchor_interval) > 0:
            if anchor_image:
                command.extend(["--anchor-image", str(anchor_image)])
            command.extend(["--anchor-interval", str(int(anchor_interval))])
            command.extend(["--anchor-strength", str(float(anchor_strength))])
            if anchor_decay and anchor_decay != "none":
                command.extend(["--anchor-decay", str(anchor_decay)])

        # User LoRA
        if user_lora and user_lora != "None" and lora_folder:
            lora_path = os.path.join(lora_folder, user_lora)
            if os.path.exists(lora_path):
                command.extend(["--lora", lora_path, str(user_lora_strength)])

        # Flags
        if disable_audio:
            command.append("--disable-audio")
        if enhance_prompt:
            command.append("--enhance-prompt")
        if offload:
            command.append("--offload")
        if enable_fp8:
            command.append("--enable-fp8")
        if distilled_checkpoint:
            command.append("--distilled-checkpoint")
        # Block swap controls (separate for DiT, text encoder, and refiner)
        if enable_dit_block_swap:
            command.append("--enable-dit-block-swap")
            command.extend(["--dit-blocks-in-memory", str(int(dit_blocks_in_memory))])
        if enable_text_encoder_block_swap:
            command.append("--enable-text-encoder-block-swap")
            command.extend(["--text-encoder-blocks-in-memory", str(int(text_encoder_blocks_in_memory))])
        if enable_refiner_block_swap:
            command.append("--enable-refiner-block-swap")
            command.extend(["--refiner-blocks-in-memory", str(int(refiner_blocks_in_memory))])

        # Preview generation
        unique_preview_suffix = f"ltx_{run_id}"
        preview_mp4_path = None
        if enable_preview and preview_interval > 0:
            preview_base_dir = os.path.join(save_path, "previews")
            command.extend(["--preview-dir", preview_base_dir])
            command.extend(["--preview-interval", str(int(preview_interval))])
            command.extend(["--preview-suffix", unique_preview_suffix])
            preview_mp4_path = os.path.join(preview_base_dir, f"latent_preview_{unique_preview_suffix}.mp4")

        # Frame freezing (video continuation)
        if freeze_frames and int(freeze_frames) > 0:
            command.extend(["--freeze-frames", str(int(freeze_frames))])
            command.extend(["--freeze-transition", str(int(freeze_transition))])

        # Sliding window (long video) - requires explicit enable flag
        if enable_sliding_window:
            command.append("--enable-sliding-window")
            if sliding_window_size and int(sliding_window_size) > 0:
                command.extend(["--sliding-window-size", str(int(sliding_window_size))])
            command.extend(["--sliding-window-overlap", str(int(sliding_window_overlap))])
            if sliding_window_overlap_noise and float(sliding_window_overlap_noise) > 0:
                command.extend(["--sliding-window-overlap-noise", str(float(sliding_window_overlap_noise))])
            if sliding_window_color_correction and float(sliding_window_color_correction) > 0:
                command.extend(["--sliding-window-color-correction", str(float(sliding_window_color_correction))])

        # Print command for debugging
        print("\n" + "=" * 80)
        print(f"LAUNCHING COMMAND (Batch {i+1}/{batch_size}):")
        print(" ".join(command))
        print("=" * 80 + "\n")

        try:
            start_time = time.perf_counter()

            process = subprocess.Popen(
                command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )

            current_process = process
            last_progress = ""
            current_preview_list = []
            last_preview_mtime = 0

            while True:
                if stop_event.is_set():
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    current_process = None
                    yield all_generated_videos, [], "Generation stopped by user.", ""
                    return

                # Read output line by line
                line = process.stdout.readline()
                if line:
                    line = line.strip()
                    if line:
                        print(line)
                        parsed = parse_ltx_progress_line(line)
                        if parsed:
                            last_progress = parsed

                # Check for preview updates
                if enable_preview and preview_mp4_path:
                    if os.path.exists(preview_mp4_path):
                        current_mtime = os.path.getmtime(preview_mp4_path)
                        if current_mtime > last_preview_mtime:
                            current_preview_list = [preview_mp4_path]
                            last_preview_mtime = current_mtime

                yield all_generated_videos.copy(), current_preview_list, status_text, last_progress

                if process.poll() is not None:
                    # Read remaining output
                    for line in process.stdout:
                        line = line.strip()
                        if line:
                            print(line)
                            parsed = parse_ltx_progress_line(line)
                            if parsed:
                                last_progress = parsed
                    break

            current_process = None
            return_code = process.returncode
            elapsed = time.perf_counter() - start_time

            if return_code == 0 and os.path.exists(output_filename):
                # Metadata is now saved by the backend (ltx_generate_video.py)
                # embedded directly in the video file's comment metadata

                label = f"Seed: {current_seed} | {elapsed:.1f}s"
                all_generated_videos.append((output_filename, label))

                status_text = f"Completed {i+1}/{batch_size}"
                yield all_generated_videos.copy(), current_preview_list, status_text, f"Video saved: {os.path.basename(output_filename)}"
            else:
                error_msg = f"Generation failed (return code: {return_code})"
                yield all_generated_videos.copy(), current_preview_list, error_msg, "Check logs for details"

        except Exception as e:
            current_process = None
            yield all_generated_videos, [], f"Error: {str(e)}", ""
            return

    final_status = f"Completed {batch_size} video(s)" if batch_size > 1 else "Generation complete!"
    yield all_generated_videos, [], final_status, "Done!"


def stop_generation():
    """Signal to stop the current generation."""
    global stop_event
    stop_event.set()
    return "Stopping generation..."


# =============================================================================
# SVI-LTX Frame Checking Functions
# =============================================================================

def variance_of_laplacian(image):
    """Calculate image sharpness using Laplacian variance."""
    import cv2
    return cv2.Laplacian(image, cv2.CV_64F).var()


def extract_best_transition_frame(video_path: str, frames_to_check: int = 30) -> int:
    """
    Find the sharpest frame from the last N frames for smooth transition.
    Uses Laplacian variance as sharpness metric.

    Args:
        video_path: Path to video file
        frames_to_check: Number of frames from end to analyze

    Returns:
        Index of the best (sharpest) frame
    """
    import cv2
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return -1

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    start_frame = max(0, total_frames - frames_to_check)

    best_frame_idx = total_frames - 1
    best_sharpness = -1

    for frame_idx in range(start_frame, total_frames):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            continue
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        sharpness = variance_of_laplacian(gray)
        if sharpness > best_sharpness:
            best_sharpness = sharpness
            best_frame_idx = frame_idx

    cap.release()
    return best_frame_idx


# =============================================================================
# SVI-LTX Video Generation Handler
# =============================================================================

def generate_svi_ltx_video(
    # Prompts (8 clips)
    prompt1: str, prompt2: str, prompt3: str, prompt4: str,
    prompt5: str, prompt6: str, prompt7: str, prompt8: str,
    negative_prompt: str,
    # Images
    input_image: str,
    anchor_image: str,
    # Video extension
    extend_video: str,
    frames_to_check: int,
    prepend_original: bool,
    # SVI settings
    num_clips: int,
    overlap_frames: int,
    num_motion_latent: int,
    num_motion_frame: int,
    seed_multiplier: int,
    # Generation params
    width: int,
    height: int,
    num_frames: int,
    frame_rate: float,
    inference_steps: int,
    cfg_scale: float,
    seed: int,
    # Anchor settings
    anchor_interval: int,
    anchor_strength: float,
    anchor_decay: str,
    # Model settings
    checkpoint_path: str,
    gemma_root: str,
    spatial_upsampler: str,
    distilled_lora: str,
    distilled_lora_strength: float,
    one_stage: bool,
    enable_fp8: bool,
    offload: bool,
    enable_dit_block_swap: bool,
    dit_blocks_in_memory: int,
    enable_text_encoder_block_swap: bool,
    text_encoder_blocks_in_memory: int,
    enable_refiner_block_swap: bool,
    refiner_blocks_in_memory: int,
    # LoRA
    lora_folder: str,
    user_lora: str,
    lora_strength: float,
    # Output
    disable_audio: bool,
    output_path: str,
    batch_size: int,
    # Preview Generation
    enable_preview: bool,
    preview_interval: int,
) -> Generator[Tuple[List[Tuple[str, str]], Optional[str], str, str], None, None]:
    """
    Generate multi-clip SVI video using LTX-2 pipeline.

    Yields: (gallery_items, preview_path, status_text, progress_text)
    """
    global stop_event, current_process, current_output_filename
    stop_event.clear()
    current_process = None
    current_output_filename = None

    # Validate inputs
    if not extend_video and not input_image:
        yield [], None, "Error: Input image required for SVI multi-clip mode", ""
        return

    error = validate_num_frames(int(num_frames))
    if error:
        yield [], None, error, ""
        return

    error = validate_resolution(int(width), int(height))
    if error:
        yield [], None, error, ""
        return

    # Round num_frames to valid 8*K+1 format
    num_frames = int(num_frames)
    k = max(1, round((num_frames - 1) / 8))
    num_frames = 8 * k + 1

    os.makedirs(output_path, exist_ok=True)
    all_generated_videos = []

    for batch_idx in range(int(batch_size)):
        if stop_event.is_set():
            current_process = None
            yield all_generated_videos, None, "Generation stopped by user.", ""
            return

        # Seed handling
        current_seed = seed
        if seed == -1:
            current_seed = random.randint(0, 2**32 - 1)
        elif int(batch_size) > 1:
            current_seed = seed + batch_idx * 1000

        # Collect per-clip prompts (non-empty only)
        all_prompts = [prompt1, prompt2, prompt3, prompt4, prompt5, prompt6, prompt7, prompt8]
        clip_prompts = [p.strip() for p in all_prompts if p and p.strip()]
        main_prompt = clip_prompts[0] if clip_prompts else "A beautiful video"

        status_text = f"Processing SVI {batch_idx+1}/{batch_size} ({num_clips} clips, Seed: {current_seed})"
        yield all_generated_videos.copy(), None, status_text, "Starting SVI generation..."

        timestamp = int(time.time())
        run_id = f"{timestamp}_{random.randint(1000, 9999)}"
        mode_str = "svi_extend" if extend_video else "svi"
        output_filename = os.path.join(output_path, f"ltx_{mode_str}_{timestamp}_{current_seed}.mp4")
        current_output_filename = output_filename

        # Frame checking for video extension
        if extend_video:
            best_frame = extract_best_transition_frame(extend_video, int(frames_to_check))
            print(f">>> Best transition frame: {best_frame}")

        # Build command
        command = [
            sys.executable, "ltx_generate_video.py",
            "--checkpoint-path", checkpoint_path,
            "--gemma-root", gemma_root,
            "--prompt", str(main_prompt),
            "--negative-prompt", str(negative_prompt),
            "--num-frames", str(int(num_frames)),
            "--frame-rate", str(float(frame_rate)),
            "--width", str(int(width)),
            "--height", str(int(height)),
            "--cfg-guidance-scale", str(float(cfg_scale)),
            "--num-inference-steps", str(int(inference_steps)),
            "--seed", str(current_seed),
            "--output-path", output_filename,
        ]

        # SVI mode or video extension
        if extend_video:
            command.extend(["--extend-video", str(extend_video)])
            if prepend_original:
                command.append("--prepend-original")
        else:
            command.append("--svi-mode")
            if input_image:
                command.extend(["--image", str(input_image), "0", "0.95"])

        # SVI settings
        command.extend([
            "--num-clips", str(int(num_clips)),
            "--num-motion-latent", str(int(num_motion_latent)),
            "--num-motion-frame", str(int(num_motion_frame)),
            "--seed-multiplier", str(int(seed_multiplier)),
            "--overlap-frames", str(int(overlap_frames)),
        ])

        # Per-clip prompts (if more than one provided)
        if len(clip_prompts) > 1:
            command.append("--prompt-list")
            command.extend(clip_prompts)

        # Anchor settings
        if anchor_image:
            command.extend(["--anchor-image", str(anchor_image)])
        if anchor_interval and int(anchor_interval) > 0:
            command.extend([
                "--anchor-interval", str(int(anchor_interval)),
                "--anchor-strength", str(float(anchor_strength)),
            ])
            if anchor_decay and anchor_decay != "none":
                command.extend(["--anchor-decay", str(anchor_decay)])

        # Model settings
        if not one_stage:
            if spatial_upsampler and os.path.exists(spatial_upsampler):
                command.extend(["--spatial-upsampler-path", spatial_upsampler])
            if distilled_lora and os.path.exists(distilled_lora):
                command.extend(["--distilled-lora", distilled_lora, str(distilled_lora_strength)])
        else:
            command.append("--one-stage")

        if enable_fp8:
            command.append("--enable-fp8")
        if offload:
            command.append("--offload")

        # Block swap settings
        if enable_dit_block_swap:
            command.append("--enable-dit-block-swap")
            command.extend(["--dit-blocks-in-memory", str(int(dit_blocks_in_memory))])
        if enable_text_encoder_block_swap:
            command.append("--enable-text-encoder-block-swap")
            command.extend(["--text-encoder-blocks-in-memory", str(int(text_encoder_blocks_in_memory))])
        if enable_refiner_block_swap:
            command.append("--enable-refiner-block-swap")
            command.extend(["--refiner-blocks-in-memory", str(int(refiner_blocks_in_memory))])

        # User LoRA
        if user_lora and user_lora != "None" and lora_folder:
            lora_path = os.path.join(lora_folder, user_lora)
            if os.path.exists(lora_path):
                command.extend(["--lora", lora_path, str(lora_strength)])

        # Audio
        if disable_audio:
            command.append("--disable-audio")

        # Preview generation
        unique_preview_suffix = f"svi_{run_id}"
        preview_mp4_path = None
        if enable_preview and preview_interval > 0:
            preview_base_dir = os.path.join(output_path, "previews")
            command.extend(["--preview-dir", preview_base_dir])
            command.extend(["--preview-interval", str(int(preview_interval))])
            command.extend(["--preview-suffix", unique_preview_suffix])
            preview_mp4_path = os.path.join(preview_base_dir, f"latent_preview_{unique_preview_suffix}.mp4")

        # Print command for debugging
        print("\n" + "=" * 80)
        print(f"LAUNCHING SVI COMMAND (Batch {batch_idx+1}/{batch_size}):")
        print(" ".join(command))
        print("=" * 80 + "\n")

        try:
            start_time = time.perf_counter()

            # Get script directory to ensure ltx_generate_video.py is found
            script_dir = os.path.dirname(os.path.abspath(__file__))
            print(f">>> Running from: {script_dir}")

            process = subprocess.Popen(
                command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                cwd=script_dir  # Run from the script directory
            )

            current_process = process
            last_progress = ""
            current_preview_list = []
            last_preview_mtime = 0

            while True:
                if stop_event.is_set():
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    current_process = None
                    yield all_generated_videos, [], "Generation stopped by user.", ""
                    return

                # Read output line by line
                line = process.stdout.readline()
                if line:
                    line = line.strip()
                    if line:
                        print(line)
                        progress_info = parse_ltx_progress_line(line)
                        if progress_info:
                            last_progress = progress_info

                # Check for preview updates
                if enable_preview and preview_mp4_path:
                    if os.path.exists(preview_mp4_path):
                        current_mtime = os.path.getmtime(preview_mp4_path)
                        if current_mtime > last_preview_mtime:
                            current_preview_list = [preview_mp4_path]
                            last_preview_mtime = current_mtime

                exit_code = process.poll()

                if exit_code is not None:
                    # Read remaining output
                    for line in process.stdout:
                        line = line.strip()
                        if line:
                            print(line)
                            progress_info = parse_ltx_progress_line(line)
                            if progress_info:
                                last_progress = progress_info

                    if exit_code == 0 and os.path.exists(output_filename):
                        elapsed = time.perf_counter() - start_time
                        label = f"SVI {num_clips} clips ({elapsed:.1f}s)"
                        all_generated_videos.append((output_filename, label))
                        yield all_generated_videos.copy(), current_preview_list, f"Completed batch {batch_idx+1}/{batch_size}", "Done!"
                    else:
                        error_msg = f"Error in batch {batch_idx+1} (exit code {exit_code})"
                        yield all_generated_videos, current_preview_list, error_msg, ""
                    break

                yield all_generated_videos.copy(), current_preview_list, status_text, last_progress

        except Exception as e:
            yield all_generated_videos, [], f"Error: {str(e)}", ""
            continue

    final_status = f"Completed {batch_size} SVI video(s)" if batch_size > 1 else "SVI generation complete!"
    yield all_generated_videos, [], final_status, "Done!"


# =============================================================================
# Gradio Interface
# =============================================================================

def create_interface():
    with gr.Blocks(
        theme=themes.Default(
            primary_hue=colors.Color(
                name="ltx_blue",
                c50="#E6F4FF",
                c100="#BAE0FF",
                c200="#91CAFF",
                c300="#69B4FF",
                c400="#409EFF",
                c500="#1677FF",
                c600="#0958D9",
                c700="#003EB3",
                c800="#002C8C",
                c900="#001D66",
                c950="#000F33"
            )
        ),
        css="""
        .green-btn {
            background: linear-gradient(to bottom right, #2ecc71, #27ae60) !important;
            color: white !important;
            border: none !important;
        }
        .green-btn:hover {
            background: linear-gradient(to bottom right, #27ae60, #219651) !important;
        }
        """,
        title="LTX-2 Video Generator"
    ) as demo:

        gr.Markdown("# LTX-2 Video Generator")
        gr.Markdown("Two-stage pipeline with joint audio-video generation")

        with gr.Tabs() as tabs:
            # =================================================================
            # Generation Tab
            # =================================================================
            with gr.Tab("Generation", id="gen_tab"):
                with gr.Row():
                    # Left column - Inputs
                    with gr.Column(scale=3):
                        prompt = gr.Textbox(
                            label="Prompt",
                            placeholder="Describe the video you want to generate...",
                            lines=4,
                            value="A giant brown capybara wearing white wireless earbuds stands upright on a Miami beach, mouth wide open revealing large front teeth, swaying its round body rhythmically side to side. Beside it, a gray tabby cat with green eyes wears a pink knit beanie with pom-pom, black bow tie, light blue t-shirt with small animal graphic, and blue jeans, pumping two bright green glow sticks overhead in time with pulsing electronic beats. The duo bounces and grooves together on golden sand scattered with seashells, their movements becoming more energetic as synthesizer drops hit. Palm trees sway in the warm breeze behind them, with Miami high-rise buildings visible in the distance. The sky transitions dramatically from warm orange and pink sunset hues to deep purple twilight, as a bright full moon rises over the turquoise ocean waves. Colorful beach umbrellas in red and white dot the background. The camera circles slowly around the dancing pair in a smooth 360-degree arc, capturing their joyful expressions. Lens flares catch the fading sunlight, then neon glow stick trails create light streaks as darkness falls. The scene pulses with energy as city lights begin twinkling along the coastline, waves crashing rhythmically in the moonlit background."
                        )
                        negative_prompt = gr.Textbox(
                            label="Negative Prompt",
                            value="blurry, low quality, distorted, artifacts, ugly, deformed",
                            lines=2
                        )

                        with gr.Row():
                            batch_size = gr.Number(label="Batch Count", value=1, minimum=1, step=1, scale=1)
                            seed = gr.Number(label="Seed (-1 = random)", value=-1, scale=1)
                            random_seed_btn = gr.Button("ðŸŽ²", scale=0, min_width=40)

                    # Right column - Status
                    with gr.Column(scale=1):
                        status_text = gr.Textbox(label="Status", interactive=False, value="Ready")
                        progress_text = gr.Textbox(label="Progress", interactive=False, value="")

                with gr.Row():
                    generate_btn = gr.Button("ðŸŽ¬ Generate Video", variant="primary", elem_classes="green-btn")
                    stop_btn = gr.Button("â¹ï¸ Stop", variant="stop")

                with gr.Row():
                    # Left column - Parameters
                    with gr.Column():
                        # Generation Parameters
                        with gr.Accordion("Generation Parameters", open=True):
                            with gr.Row():
                                mode = gr.Dropdown(
                                    label="Mode",
                                    choices=["t2v", "i2v", "v2v"],
                                    value="t2v",
                                    info="t2v = text-to-video, i2v = image-to-video, v2v = video-to-video (refine)"
                                )
                                pipeline = gr.Dropdown(
                                    label="Pipeline",
                                    choices=["two-stage", "one-stage", "refine-only"],
                                    value="two-stage",
                                    info="two-stage = higher quality, one-stage = faster, refine-only = stage 2 only on input video"
                                )
                                enable_sliding_window = gr.Checkbox(
                                    label="Sliding Window",
                                    value=False,
                                    info="Enable for long videos (>129 frames)"
                                )
                            # Hidden state for original image/video dimensions
                            original_dims = gr.State(value="")
                        # Image Conditioning (I2V)
                        with gr.Accordion("Image Conditioning (I2V)", open=True) as i2v_section:
                            input_image = gr.Image(label="Start Image", type="filepath")
                            with gr.Row():
                                image_frame_idx = gr.Number(label="Frame Index", value=0, minimum=0, info="Which frame to condition (0 = first)")
                                image_strength = gr.Slider(minimum=0.0, maximum=1.0, value=0.9, step=0.05, label="Strength")

                            with gr.Accordion("Anchor Image (periodic guidance)", open=False):
                                gr.Markdown("Inject the anchor image at regular intervals to guide the video generation.")
                                anchor_image = gr.Image(label="Anchor Image (optional, uses Start Image if empty)", type="filepath")
                                with gr.Row():
                                    anchor_interval = gr.Number(
                                        label="Anchor Interval",
                                        value=0,
                                        minimum=0,
                                        step=8,
                                        info="Frame interval (e.g., 60). Set to 0 to disable."
                                    )
                                    anchor_strength = gr.Slider(
                                        minimum=0.0, maximum=1.0, value=0.1, step=0.01,
                                        label="Anchor Strength",
                                        info="How strongly to guide toward anchor"
                                    )
                                    anchor_decay = gr.Dropdown(
                                        label="Anchor Decay",
                                        choices=["none", "linear", "cosine", "sigmoid"],
                                        value="cosine",
                                        info="Decay schedule: strong early, weak later for motion"
                                    )

                            with gr.Accordion("End Image (optional)", open=False):
                                end_image = gr.Image(label="End Image (for start-to-end video)", type="filepath")
                                gr.Markdown("Set an ending frame to generate video that transitions from start to end image.")
                                with gr.Row():
                                    end_image_strength = gr.Slider(minimum=0.0, maximum=1.0, value=0.9, step=0.05, label="End Image Strength")
                            gr.Markdown("### Resolution Settings")
                            scale_slider = gr.Slider(
                                minimum=1, maximum=200, value=100, step=1,
                                label="Scale % (adjusts resolution while maintaining aspect ratio)",
                                info="Scale the input image dimensions. Works for I2V mode."
                            )
                            with gr.Row():
                                width = gr.Number(label="Width", value=1024, step=64, info="Must be divisible by 64")
                                calc_height_btn = gr.Button("â†’", size="sm", min_width=40)
                                calc_width_btn = gr.Button("â†", size="sm", min_width=40)
                                height = gr.Number(label="Height", value=1024, step=64, info="Must be divisible by 64")
                            with gr.Row():
                                num_frames = gr.Slider(minimum=9, maximum=2001, step=8, value=121, label="Num Frames (8*K+1)", info="e.g., 121 = 5s @ 24fps")
                                frame_rate = gr.Slider(minimum=12, maximum=60, value=24, step=1, label="Frame Rate")
                            with gr.Row():
                                cfg_guidance_scale = gr.Slider(minimum=1.0, maximum=15.0, value=4.0, step=0.5, label="CFG Scale")
                                num_inference_steps = gr.Slider(minimum=1, maximum=60, value=40, step=1, label="Inference Steps")
                                stage2_steps = gr.Slider(minimum=1, maximum=60, value=3, step=1, label="Stage 2 Steps")

                        # Video Input (V2V / Refine)
                        with gr.Accordion("Video Input (V2V / Refine)", open=False) as v2v_section:
                            input_video = gr.Video(label="Input Video (for refinement)", sources=["upload"])
                            gr.Markdown("""
                            **Video-to-Video Refinement:**
                            - Upload a video to refine it using stage 2 (with distilled LoRA)
                            - Use "refine-only" pipeline to skip stage 1 generation
                            - Resolution and frame count will be auto-detected from video
                            """)
                            with gr.Row():
                                refine_strength = gr.Slider(
                                    minimum=0.0, maximum=1.0, value=0.3, step=0.05,
                                    label="Refine Strength",
                                    info="Amount of noise to add before refinement (0=none, 1=full denoise)"
                                )
                                refine_steps = gr.Slider(
                                    minimum=1, maximum=30, value=10, step=1,
                                    label="Refine Steps",
                                    info="Number of refinement denoising steps"
                                )

                        # Video Continuation (Frame Freezing)
                        with gr.Accordion("Video Continuation (Frame Freezing)", open=False):
                            gr.Markdown("Freeze first N frames from input video during denoising for smooth continuation.")
                            with gr.Row():
                                freeze_frames = gr.Slider(
                                    minimum=0, maximum=32, value=0, step=1,
                                    label="Freeze Frames",
                                    info="Number of frames to freeze from input video (0 = disabled)"
                                )
                                freeze_transition = gr.Slider(
                                    minimum=1, maximum=16, value=4, step=1,
                                    label="Transition Frames",
                                    info="Frames for gradual blend from frozen to generated"
                                )

                        # Sliding Window (Long Video)
                        with gr.Accordion("Sliding Window (Long Video)", open=False):
                            gr.Markdown("Generate videos longer than the model's context window by overlapping windows.")
                            with gr.Row():
                                sliding_window_size = gr.Slider(
                                    minimum=0, maximum=257, value=0, step=8,
                                    label="Window Size",
                                    info="Frames per window (8n+1). 0 = auto-detect from num_frames"
                                )
                                sliding_window_overlap = gr.Slider(
                                    minimum=1, maximum=33, value=9, step=8,
                                    label="Overlap Frames",
                                    info="Overlapping frames between windows (8n+1)"
                                )
                            with gr.Row():
                                sliding_window_overlap_noise = gr.Slider(
                                    minimum=0.0, maximum=100.0, value=0.0, step=5.0,
                                    label="Overlap Noise %",
                                    info="Noise level for overlap blending (0 = no noise)"
                                )
                                sliding_window_color_correction = gr.Slider(
                                    minimum=0.0, maximum=1.0, value=0.0, step=0.1,
                                    label="Color Correction",
                                    info="LAB color correction strength between windows"
                                )


                    # Right column - Output
                    with gr.Column():
                        output_gallery = gr.Gallery(
                            label="Generated Videos",
                            columns=2, rows=2,
                            object_fit="contain",
                            height="auto",
                            allow_preview=True,
                            preview=True
                        )
                        # Latent Preview (During Generation)
                        with gr.Accordion("Latent Preview (During Generation)", open=True):
                            enable_preview = gr.Checkbox(label="Enable Latent Preview", value=True)
                            preview_interval = gr.Slider(
                                minimum=1, maximum=50, step=1, value=5,
                                label="Preview Every N Steps"
                            )
                            preview_gallery = gr.Gallery(
                                label="Latent Previews",
                                columns=4, rows=2,
                                object_fit="contain",
                                height=300,
                                allow_preview=True,
                                preview=True,
                                show_label=True
                            )
                        # User LoRA
                        with gr.Accordion("User LoRA (Optional)", open=False):
                            lora_folder = gr.Textbox(label="LoRA Folder", value="lora")
                            lora_refresh_btn = gr.Button("ðŸ”„ Refresh", size="sm")
                            with gr.Row():
                                user_lora = gr.Dropdown(
                                    label="LoRA",
                                    choices=get_ltx_lora_options("lora"),
                                    value="None",
                                    scale=3
                                )
                                user_lora_strength = gr.Slider(
                                    minimum=0.0, maximum=2.0, value=1.0, step=0.1,
                                    label="Strength", scale=1
                                )
                        # Memory Optimization
                        with gr.Accordion("Model settings", open=True):
                            with gr.Row():
                                offload = gr.Checkbox(label="CPU Offloading", value=False, info="Offload models to CPU when not in use")
                                enable_fp8 = gr.Checkbox(label="FP8 Mode", value=False, info="Reduce memory with FP8 transformer")
                            gr.Markdown("### Block Swapping")
                            with gr.Row():
                                enable_dit_block_swap = gr.Checkbox(label="DiT Block Swap", value=True, info="Main transformer (stage 1)")
                                dit_blocks_in_memory = gr.Slider(minimum=1, maximum=47, value=22, step=1, label="DiT Blocks in GPU", visible=True)
                            with gr.Row():
                                enable_text_encoder_block_swap = gr.Checkbox(label="Text Encoder Block Swap", value=True, info="Gemma text encoder")
                                text_encoder_blocks_in_memory = gr.Slider(minimum=1, maximum=47, value=6, step=1, label="Text Encoder Blocks in GPU", visible=True, info="Gemma-3-12B has 48 layers")
                            with gr.Row():
                                enable_refiner_block_swap = gr.Checkbox(label="Refiner Block Swap", value=True, info="Stage 2 refiner transformer")
                                refiner_blocks_in_memory = gr.Slider(minimum=1, maximum=47, value=22, step=1, label="Refiner Blocks in GPU", visible=True)
                            with gr.Row():
                                disable_audio = gr.Checkbox(label="Disable Audio", value=False, info="Generate video only (no audio)")
                                enhance_prompt = gr.Checkbox(label="Enhance Prompt", value=False, info="Use Gemma to improve prompt")
                            with gr.Row():
                                checkpoint_path = gr.Textbox(
                                    label="LTX Checkpoint Path",
                                    value="./weights/ltx-2-19b-dev.safetensors",
                                    info="Path to LTX-2 model checkpoint",
                                    scale=4
                                )
                                distilled_checkpoint = gr.Checkbox(
                                    label="Distilled",
                                    value=False,
                                    info="Checkpoint is distilled (skips distilled LoRA)",
                                    scale=1
                                )
                            stage2_checkpoint = gr.Textbox(
                                label="Stage 2 Checkpoint (optional)",
                                value="",
                                info="Full model checkpoint for stage 2 refinement (leave empty to use distilled LoRA)",
                                placeholder="e.g., ./weights/ltx-2-19b-distilled.safetensors"
                            )
                            gemma_root = gr.Textbox(
                                label="Gemma Root Path",
                                value="./gemma-3-12b-it-qat-q4_0-unquantized",
                                info="Path to Gemma text encoder"
                            )
                            spatial_upsampler_path = gr.Textbox(
                                label="Spatial Upsampler Path",
                                value="./weights/ltx-2-spatial-upscaler-x2-1.0.safetensors",
                                info="Path to 2x spatial upsampler"
                            )
                            with gr.Row():
                                distilled_lora_path = gr.Textbox(
                                    label="Distilled LoRA Path",
                                    value="./weights/ltx-2-19b-distilled-lora-384.safetensors",
                                    info="For stage 2 refinement",
                                    scale=3
                                )
                                distilled_lora_strength = gr.Slider(
                                    minimum=0.0, maximum=2.0, value=1.0, step=0.1,
                                    label="Strength", scale=1
                                )
                            save_path = gr.Textbox(label="Output Folder", value="outputs")
                            with gr.Row():
                                lt1_save_defaults_btn = gr.Button("Save Defaults")
                                lt1_load_defaults_btn = gr.Button("Load Defaults")
                            lt1_defaults_status = gr.Textbox(label="Defaults Status", interactive=False, visible=False)

            # =================================================================
            # Video Info Tab
            # =================================================================
            with gr.Tab("Video Info"):
                with gr.Row():
                    info_video_input = gr.Video(label="Upload Video", interactive=True)
                    info_first_frame = gr.Image(label="First Frame", interactive=False)
                    info_metadata_output = gr.JSON(label="Generation Parameters")

                with gr.Row():
                    info_status = gr.Textbox(label="Status", interactive=False)

                with gr.Row():
                    info_send_btn = gr.Button("Send to Generation", variant="primary")

            # =================================================================
            # SVI-LTX Tab
            # =================================================================
            with gr.Tab("SVI-LTX", id="svi_ltx_tab"):
                gr.Markdown("""
                ## SVI-LTX (Stable-Video-Infinity for LTX)
                Generate long, consistent videos by chaining multiple clips. Each clip uses motion latents from the previous clip for smooth transitions.
                **Features:** Multi-clip streaming â€¢ Motion latent conditioning â€¢ Per-clip prompts â€¢ Video extension
                """)

                # Clip Prompts - Two Column Layout per Accordion
                with gr.Accordion("Clip Prompts (1-4)", open=True):
                    with gr.Row():
                        with gr.Column():
                            svi_prompt1 = gr.Textbox(
                                label="Clip 1 Prompt (Required)", lines=2,
                                value="A fluffy cat and a curious rabbit discover a miniature fat man doll on the living room floor."
                            )
                            svi_prompt2 = gr.Textbox(
                                label="Clip 2 Prompt", lines=2,
                                value="The cat bats the tiny fat man doll with its paw while the rabbit hops around excitedly."
                            )
                        with gr.Column():
                            svi_prompt3 = gr.Textbox(
                                label="Clip 3 Prompt", lines=2,
                                value="The rabbit picks up the miniature fat man doll in its mouth and runs away, the cat chasing after it."
                            )
                            svi_prompt4 = gr.Textbox(
                                label="Clip 4 Prompt", lines=2,
                                value="The cat and rabbit sit together, both gazing at the tiny fat man doll between them."
                            )

                with gr.Accordion("Additional Clips (5-8)", open=False):
                    with gr.Row():
                        with gr.Column():
                            svi_prompt5 = gr.Textbox(label="Clip 5 Prompt", lines=2, value="")
                            svi_prompt6 = gr.Textbox(label="Clip 6 Prompt", lines=2, value="")
                        with gr.Column():
                            svi_prompt7 = gr.Textbox(label="Clip 7 Prompt", lines=2, value="")
                            svi_prompt8 = gr.Textbox(label="Clip 8 Prompt", lines=2, value="")

                with gr.Row():
                    with gr.Column(scale=4):
                        svi_negative_prompt = gr.Textbox(
                            label="Negative Prompt",
                            value="blurry, low quality, distorted, artifacts, ugly, deformed",
                            lines=2,
                        )
                    with gr.Column(scale=1):
                        svi_batch_size = gr.Number(label="Batch Count", value=1, minimum=1, step=1)
                    with gr.Column(scale=2):
                        svi_batch_progress = gr.Textbox(label="Status", interactive=False, value="")
                        svi_progress_text = gr.Textbox(label="Progress", interactive=False, value="")

                with gr.Row():
                    svi_generate_btn = gr.Button("ðŸŽ¬ Generate SVI Video", variant="primary", elem_classes="green-btn")
                    svi_stop_btn = gr.Button("â¹ï¸ Stop", variant="stop")

                with gr.Row():
                    with gr.Column():
                        # Input images for SVI
                        with gr.Row():
                            svi_input_image = gr.Image(label="Input Image (Required)", type="filepath")
                            svi_anchor_image = gr.Image(label="Anchor Image (Optional)", type="filepath")
                        svi_original_dims = gr.Textbox(label="Original Dimensions", interactive=False, visible=False)

                        # Video Extension Mode
                        with gr.Accordion("Video Extension (Extend Existing Video)", open=False):
                            gr.Markdown("Extend an existing video by finding the best transition frame and generating new clips from it.")
                            svi_extend_video = gr.Video(label="Video to Extend", sources=["upload"])
                            with gr.Row():
                                svi_frames_to_check = gr.Slider(
                                    minimum=1, maximum=100, step=1, value=30,
                                    label="Frames to Check",
                                    info="Analyze last N frames to find sharpest transition point (1 = use last frame)"
                                )
                                svi_prepend_original = gr.Checkbox(
                                    label="Prepend Original",
                                    value=True,
                                    info="Include original video before extension"
                                )

                        # SVI-specific settings
                        gr.Markdown("### SVI Multi-Clip Settings")
                        with gr.Row():
                            svi_num_clips = gr.Slider(minimum=1, maximum=8, step=1, label="Number of Clips", value=4,
                                                     info="How many clips to chain together")
                            svi_overlap_frames = gr.Slider(minimum=0, maximum=16, step=1, label="Overlap Frames", value=1,
                                                          info="Overlapping frames between clips")
                            svi_num_motion_latent = gr.Slider(minimum=0, maximum=4, step=1, label="Motion Latent Frames", value=2,
                                                              info="Latent frames from previous clip for motion context")
                        with gr.Row():
                            svi_num_motion_frame = gr.Slider(minimum=1, maximum=8, step=1, label="Motion Frame Offset", value=1,
                                                              info="Frame offset from end for next clip input (1=last frame)")
                            svi_seed_multiplier = gr.Slider(minimum=1, maximum=200, step=1, label="Seed Multiplier", value=42,
                                                             info="Per-clip seed variation (seed = base + clip * multiplier)")

                        gr.Markdown("### Generation Parameters")
                        svi_scale_slider = gr.Slider(minimum=1, maximum=200, value=100, step=1, label="Scale %")
                        with gr.Row():
                            svi_width = gr.Number(label="Width", value=768, step=64, interactive=True)
                            svi_calc_height_btn = gr.Button("â†’")
                            svi_calc_width_btn = gr.Button("â†")
                            svi_height = gr.Number(label="Height", value=512, step=64, interactive=True)
                        svi_num_frames = gr.Slider(minimum=9, maximum=241, step=8, label="Frames Per Clip", value=121, info="Frame count for each individual clip (8k+1)")
                        svi_frame_rate = gr.Slider(minimum=1, maximum=60, step=1, label="Frames Per Second", value=24)
                        svi_inference_steps = gr.Slider(minimum=4, maximum=100, step=1, label="Sampling Steps", value=40)
                        svi_cfg_scale = gr.Slider(minimum=1.0, maximum=20.0, step=0.1, label="Guidance Scale", value=4.0)
                        with gr.Row():
                            svi_seed = gr.Number(label="Seed (-1 for random)", value=-1)
                            svi_random_seed_btn = gr.Button("ðŸŽ²")

                        with gr.Accordion("Anchor Settings", open=False):
                            gr.Markdown("Inject anchor image at regular intervals to guide video generation.")
                            svi_anchor_interval = gr.Number(
                                label="Anchor Interval",
                                value=0,
                                minimum=0,
                                step=8,
                                info="Frame interval (e.g., 60). Set to 0 to disable."
                            )
                            svi_anchor_strength = gr.Slider(
                                minimum=0.0, maximum=1.0, value=0.1, step=0.01,
                                label="Anchor Strength",
                                info="How strongly to guide toward anchor"
                            )
                            svi_anchor_decay = gr.Dropdown(
                                label="Anchor Decay",
                                choices=["none", "linear", "cosine", "sigmoid"],
                                value="cosine",
                                info="Decay schedule for anchor strength"
                            )

                    with gr.Column():
                        svi_output_gallery = gr.Gallery(
                            label="Generated Videos (Click to select)",
                            columns=[2], rows=[2], object_fit="contain", height="auto",
                            show_label=True, allow_preview=True, preview=True
                        )
                        with gr.Accordion("Preview (During Generation)", open=True):
                            svi_preview_gallery = gr.Gallery(
                                label="Clip Previews", columns=4, rows=2, object_fit="contain", height=300,
                                allow_preview=True, preview=True, show_label=True
                            )
                        with gr.Accordion("LoRA", open=True):
                            with gr.Row():
                                svi_lora_folder = gr.Textbox(label="LoRA Folder", value="lora")
                                svi_lora_refresh_btn = gr.Button("ðŸ”„ LoRA")
                            svi_lora_dropdown = gr.Dropdown(
                                label="Select LoRA",
                                choices=["None"],
                                value="None",
                                interactive=True
                            )
                            svi_lora_strength = gr.Slider(
                                minimum=0.0, maximum=2.0, step=0.05,
                                label="LoRA Strength", value=1.0
                            )

                # Model Settings
                with gr.Accordion("Model Settings", open=True):
                    with gr.Row():
                        svi_checkpoint_path = gr.Textbox(
                            label="LTX Checkpoint",
                            value="./weights/ltx-2-19b-dev.safetensors",
                            info="Main LTX model checkpoint"
                        )
                        svi_gemma_root = gr.Textbox(
                            label="Gemma Root",
                            value="./gemma-3-12b-it-qat-q4_0-unquantized",
                            info="Text encoder directory"
                        )
                    with gr.Row():
                        svi_spatial_upsampler = gr.Textbox(
                            label="Spatial Upsampler Path",
                            value="./weights/ltx-2-spatial-upscaler-x2-1.0.safetensors",
                            info="For 2x upscaling (two-stage)"
                        )
                        svi_distilled_lora = gr.Textbox(
                            label="Distilled LoRA Path",
                            value="./weights/ltx-2-19b-distilled-lora-384.safetensors",
                            info="For stage 2 refinement (two-stage)"
                        )
                    with gr.Row():
                        svi_distilled_lora_strength = gr.Slider(
                            minimum=0.0, maximum=2.0, step=0.05,
                            label="Distilled LoRA Strength", value=1.0
                        )
                    with gr.Row():
                        svi_one_stage = gr.Checkbox(label="One-Stage Pipeline", value=False, info="Skip two-stage refinement (faster but lower quality)")
                        svi_enable_fp8 = gr.Checkbox(label="Enable FP8", value=False, info="Use FP8 precision (lower VRAM)")
                        svi_offload = gr.Checkbox(label="Offload", value=False, info="Offload models to CPU")

                    # Block swap settings
                    gr.Markdown("### Block Swap Settings")
                    with gr.Row():
                        svi_enable_dit_block_swap = gr.Checkbox(label="DiT Block Swap", value=True)
                        svi_dit_blocks_in_memory = gr.Number(label="DiT Blocks in Memory", value=22, visible=True)
                    with gr.Row():
                        svi_enable_text_encoder_block_swap = gr.Checkbox(label="Text Encoder Block Swap", value=True)
                        svi_text_encoder_blocks_in_memory = gr.Number(label="Text Encoder Blocks in Memory", value=6, visible=True)
                    with gr.Row():
                        svi_enable_refiner_block_swap = gr.Checkbox(label="Refiner Block Swap", value=True)
                        svi_refiner_blocks_in_memory = gr.Number(label="Refiner Blocks in Memory", value=22, visible=True)

                    with gr.Row():
                        svi_output_path = gr.Textbox(
                            label="Output Path",
                            value="outputs",
                            info="Directory for generated videos"
                        )
                        svi_disable_audio = gr.Checkbox(label="Disable Audio", value=False, info="Skip audio generation")

                    # Preview Generation (SVI)
                    with gr.Accordion("Latent Preview (During Generation)", open=True):
                        svi_enable_preview = gr.Checkbox(label="Enable Latent Preview", value=True)
                        svi_preview_interval = gr.Slider(
                            minimum=1, maximum=50, step=1, value=5,
                            label="Preview Every N Steps"
                        )

                with gr.Row():
                    svi_save_defaults_btn = gr.Button("ðŸ’¾ Save Defaults")
                    svi_load_defaults_btn = gr.Button("ðŸ“‚ Load Defaults")

            # =================================================================
            # Help Tab
            # =================================================================
            with gr.Tab("Help"):
                gr.Markdown("""
                ## LTX-2 Video Generation Help

                ### Required Model Files
                **For two-stage pipeline** (default, higher quality):
                1. **LTX Checkpoint** - Main 19B model (.safetensors)
                2. **Gemma Root** - Text encoder directory
                3. **Spatial Upsampler** - For 2x resolution upscaling
                4. **Distilled LoRA** - For stage 2 refinement

                **For one-stage pipeline** (faster):
                1. **LTX Checkpoint** - Main 19B model (.safetensors)
                2. **Gemma Root** - Text encoder directory

                ### Pipeline Options
                - **Two-stage** (default): Generates at half resolution, upsamples 2x, then refines with distilled LoRA. Higher quality output.
                - **One-stage**: Generates directly at full resolution in a single pass. Faster but may have less detail.
                - **Refine-only**: Runs stage 2 refinement on an input video. Use this to improve a previously generated video.

                ### Stage 2 Checkpoint
                - By default, stage 2 uses the main checkpoint + distilled LoRA for refinement
                - **Stage 2 Checkpoint** (optional): Use a separate full model checkpoint for stage 2 (e.g., a distilled model)
                - When a stage 2 checkpoint is provided, the distilled LoRA is not applied

                ### Generation Modes
                - **T2V (Text-to-Video)**: Generate video from text prompt only
                - **I2V (Image-to-Video)**: Start from an input image
                - **V2V (Video-to-Video)**: Refine an existing video

                ### Video Input (V2V / Refine)
                - Upload a video to refine it using stage 2
                - Use "refine-only" pipeline to skip stage 1 generation
                - **Refine Strength**: Amount of noise added before refinement (0=none, 1=full denoise)
                - Distilled LoRA is optional for refine-only mode

                ### Image Conditioning (I2V)
                - **Start Image**: Set the first frame of the video
                - **End Image** (optional): Set the last frame to create start-to-end transitions
                - **Scale Slider**: Automatically adjusts width/height based on input image aspect ratio
                - **â†’ / â† Buttons**: Calculate width from height or height from width while preserving aspect ratio

                ### Resolution Requirements
                - Width and height must be divisible by 64
                - Recommended: 1024x1024, 768x1024, 1024x768
                - When using I2V, the scale slider helps maintain the input image's aspect ratio

                ### Frame Count
                - Must be in format: 8*K + 1
                - Examples: 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97, 105, 113, 121
                - 121 frames @ 24fps = 5 seconds

                ### Memory Optimization
                - **CPU Offloading**: Reduces peak VRAM by moving models to CPU
                - **FP8 Mode**: Stores transformer in FP8 (lower precision)
                - **Block Swapping**: Keeps only N transformer blocks in GPU (~40% VRAM reduction)

                ### Audio Generation
                - LTX-2 generates synchronized audio by default
                - Disable audio if you only need video
                """)

        # =================================================================
        # Event Handlers
        # =================================================================

        # Random seed button
        random_seed_btn.click(
            fn=lambda: -1,
            outputs=[seed]
        )

        # LoRA refresh
        lora_refresh_btn.click(
            fn=refresh_lora_dropdown,
            inputs=[lora_folder],
            outputs=[user_lora]
        )

        # Block swap visibility toggles
        enable_dit_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[enable_dit_block_swap],
            outputs=[dit_blocks_in_memory]
        )
        enable_text_encoder_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[enable_text_encoder_block_swap],
            outputs=[text_encoder_blocks_in_memory]
        )
        enable_refiner_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[enable_refiner_block_swap],
            outputs=[refiner_blocks_in_memory]
        )

        # Image dimension handlers
        input_image.change(
            fn=update_image_dimensions,
            inputs=[input_image],
            outputs=[original_dims, width, height]
        )

        scale_slider.change(
            fn=update_resolution_from_scale,
            inputs=[scale_slider, original_dims],
            outputs=[width, height]
        )

        calc_width_btn.click(
            fn=calculate_width_from_height,
            inputs=[height, original_dims],
            outputs=[width]
        )

        calc_height_btn.click(
            fn=calculate_height_from_width,
            inputs=[width, original_dims],
            outputs=[height]
        )

        # Mode change - show/hide I2V and V2V sections
        def update_sections_visibility(m):
            return gr.update(open=(m == "i2v")), gr.update(open=(m == "v2v"))

        mode.change(
            fn=update_sections_visibility,
            inputs=[mode],
            outputs=[i2v_section, v2v_section]
        )

        # Video input change - update dimensions and frame count
        input_video.change(
            fn=update_video_dimensions,
            inputs=[input_video],
            outputs=[original_dims, width, height, num_frames, frame_rate]
        )

        # Stop button
        stop_btn.click(
            fn=stop_generation,
            outputs=[status_text]
        )

        # Generate button
        generate_btn.click(
            fn=generate_ltx_video,
            inputs=[
                prompt, negative_prompt,
                checkpoint_path, distilled_checkpoint, stage2_checkpoint, gemma_root, spatial_upsampler_path,
                distilled_lora_path, distilled_lora_strength,
                mode, pipeline, enable_sliding_window, width, height, num_frames, frame_rate,
                cfg_guidance_scale, num_inference_steps, stage2_steps, seed,
                input_image, image_frame_idx, image_strength,
                end_image, end_image_strength,
                anchor_image, anchor_interval, anchor_strength, anchor_decay,
                input_video, refine_strength, refine_steps,
                disable_audio, enhance_prompt,
                offload, enable_fp8,
                enable_dit_block_swap, dit_blocks_in_memory,
                enable_text_encoder_block_swap, text_encoder_blocks_in_memory,
                enable_refiner_block_swap, refiner_blocks_in_memory,
                lora_folder, user_lora, user_lora_strength,
                save_path, batch_size,
                # Preview Generation
                enable_preview, preview_interval,
                # Video Continuation (Frame Freezing)
                freeze_frames, freeze_transition,
                # Sliding Window (Long Video)
                sliding_window_size, sliding_window_overlap,
                sliding_window_overlap_noise, sliding_window_color_correction,
            ],
            outputs=[output_gallery, preview_gallery, status_text, progress_text]
        )

        # =================================================================
        # Video Info Tab Event Handlers
        # =================================================================
        info_video_input.upload(
            fn=extract_video_details,
            inputs=info_video_input,
            outputs=[info_metadata_output, info_status, info_first_frame]
        )

        def send_to_generation_handler(metadata, first_frame):
            """Send loaded metadata to generation tab parameters and switch to Generation tab."""
            if not metadata:
                return [gr.update()] * 37 + ["No metadata loaded - upload a video first"]

            # Handle legacy metadata that used single enable_block_swap
            legacy_block_swap = metadata.get("enable_block_swap", True)

            # Extract image conditioning info from metadata
            images = metadata.get("images", [])
            image_strength = 0.9
            image_frame_idx = 0
            if images and len(images) > 0:
                # First image entry: (path, frame_idx, strength)
                image_frame_idx = images[0][1] if len(images[0]) > 1 else 0
                image_strength = images[0][2] if len(images[0]) > 2 else 0.9

            # Determine mode based on whether images were used
            mode = "t2v"
            if images:
                mode = "i2v"
            elif metadata.get("input_video"):
                mode = "v2v"

            # Return updates for all generation parameters
            # NOTE: Model paths (checkpoint_path, gemma_root, spatial_upsampler_path,
            #       distilled_lora_path) are NOT restored - user keeps their current settings
            return [
                gr.Tabs(selected="gen_tab"),  # Switch to Generation tab
                gr.update(value=metadata.get("prompt", "")),  # prompt
                gr.update(value=metadata.get("negative_prompt", "")),  # negative_prompt
                gr.update(value=mode),  # mode
                gr.update(value=metadata.get("pipeline", "two-stage")),  # pipeline
                gr.update(value=metadata.get("width", 1024)),  # width
                gr.update(value=metadata.get("height", 1024)),  # height
                gr.update(value=metadata.get("num_frames", 121)),  # num_frames
                gr.update(value=metadata.get("frame_rate", 24)),  # frame_rate
                gr.update(value=metadata.get("cfg_guidance_scale", 4.0)),  # cfg_guidance_scale
                gr.update(value=metadata.get("num_inference_steps", 40)),  # num_inference_steps
                gr.update(value=metadata.get("stage2_steps", 3)),  # stage2_steps
                gr.update(value=metadata.get("seed", -1)),  # seed
                # Image conditioning
                gr.update(value=first_frame),  # input_image - use extracted first frame
                gr.update(value=image_frame_idx),  # image_frame_idx
                gr.update(value=image_strength),  # image_strength
                gr.update(value=metadata.get("end_image_strength", 0.9)),  # end_image_strength
                # Anchor conditioning
                gr.update(value=metadata.get("anchor_interval", 0) or 0),  # anchor_interval
                gr.update(value=metadata.get("anchor_strength", 0.8)),  # anchor_strength
                gr.update(value=metadata.get("anchor_decay", "cosine") or "cosine"),  # anchor_decay
                # Refine settings
                gr.update(value=metadata.get("refine_strength", 0.3)),  # refine_strength
                gr.update(value=metadata.get("refine_steps", 10)),  # refine_steps
                # Audio and prompt
                gr.update(value=metadata.get("disable_audio", False)),  # disable_audio
                gr.update(value=metadata.get("enhance_prompt", False)),  # enhance_prompt
                # Memory optimization
                gr.update(value=metadata.get("offload", False)),  # offload
                gr.update(value=metadata.get("enable_fp8", False)),  # enable_fp8
                # Block swap settings
                gr.update(value=metadata.get("enable_dit_block_swap", legacy_block_swap)),  # enable_dit_block_swap
                gr.update(value=metadata.get("dit_blocks_in_memory", 22) or 22),  # dit_blocks_in_memory
                gr.update(value=metadata.get("enable_text_encoder_block_swap", legacy_block_swap)),  # enable_text_encoder_block_swap
                gr.update(value=metadata.get("text_encoder_blocks_in_memory", 6) or 6),  # text_encoder_blocks_in_memory
                gr.update(value=metadata.get("enable_refiner_block_swap", legacy_block_swap)),  # enable_refiner_block_swap
                gr.update(value=metadata.get("refiner_blocks_in_memory", 22) or 22),  # refiner_blocks_in_memory
                # Distilled settings (NOT model paths)
                gr.update(value=metadata.get("distilled_checkpoint", False)),  # distilled_checkpoint
                # NOTE: stage2_checkpoint path is NOT restored - keep user's current setting
                "Parameters sent to Generation tab (model paths unchanged)"  # status
            ]

        info_send_btn.click(
            fn=send_to_generation_handler,
            inputs=[info_metadata_output, info_first_frame],
            outputs=[
                tabs,  # Switch tab
                prompt, negative_prompt, mode, pipeline,
                width, height, num_frames, frame_rate,
                cfg_guidance_scale, num_inference_steps, stage2_steps, seed,
                # Image conditioning
                input_image, image_frame_idx, image_strength, end_image_strength,
                # Anchor conditioning
                anchor_interval, anchor_strength, anchor_decay,
                # Refine settings
                refine_strength, refine_steps,
                # Audio and prompt
                disable_audio, enhance_prompt,
                # Memory optimization
                offload, enable_fp8,
                # Block swap settings
                enable_dit_block_swap, dit_blocks_in_memory,
                enable_text_encoder_block_swap, text_encoder_blocks_in_memory,
                enable_refiner_block_swap, refiner_blocks_in_memory,
                # Distilled settings
                distilled_checkpoint,
                info_status
            ]
        )

        # =================================================================
        # SVI-LTX Event Handlers
        # =================================================================

        # SVI-LTX Generate button
        svi_generate_btn.click(
            fn=generate_svi_ltx_video,
            inputs=[
                # Prompts (8)
                svi_prompt1, svi_prompt2, svi_prompt3, svi_prompt4,
                svi_prompt5, svi_prompt6, svi_prompt7, svi_prompt8,
                svi_negative_prompt,
                # Images
                svi_input_image, svi_anchor_image,
                # Video extension
                svi_extend_video, svi_frames_to_check, svi_prepend_original,
                # SVI settings
                svi_num_clips, svi_overlap_frames, svi_num_motion_latent,
                svi_num_motion_frame, svi_seed_multiplier,
                # Generation params
                svi_width, svi_height, svi_num_frames, svi_frame_rate,
                svi_inference_steps, svi_cfg_scale, svi_seed,
                # Anchor settings
                svi_anchor_interval, svi_anchor_strength, svi_anchor_decay,
                # Model settings
                svi_checkpoint_path, svi_gemma_root, svi_spatial_upsampler,
                svi_distilled_lora, svi_distilled_lora_strength, svi_one_stage,
                svi_enable_fp8, svi_offload,
                # Block swap
                svi_enable_dit_block_swap, svi_dit_blocks_in_memory,
                svi_enable_text_encoder_block_swap, svi_text_encoder_blocks_in_memory,
                svi_enable_refiner_block_swap, svi_refiner_blocks_in_memory,
                # LoRA
                svi_lora_folder, svi_lora_dropdown, svi_lora_strength,
                # Output
                svi_disable_audio, svi_output_path, svi_batch_size,
                # Preview Generation
                svi_enable_preview, svi_preview_interval,
            ],
            outputs=[svi_output_gallery, svi_preview_gallery, svi_batch_progress, svi_progress_text]
        )

        # SVI-LTX Stop button
        svi_stop_btn.click(
            fn=stop_generation,
            outputs=[svi_batch_progress]
        )

        # SVI-LTX Random seed button
        svi_random_seed_btn.click(
            fn=lambda: -1,
            outputs=[svi_seed]
        )

        # SVI-LTX LoRA refresh
        svi_lora_refresh_btn.click(
            fn=refresh_lora_dropdown,
            inputs=[svi_lora_folder],
            outputs=[svi_lora_dropdown]
        )

        # SVI-LTX Block swap visibility toggles
        svi_enable_dit_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[svi_enable_dit_block_swap],
            outputs=[svi_dit_blocks_in_memory]
        )
        svi_enable_text_encoder_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[svi_enable_text_encoder_block_swap],
            outputs=[svi_text_encoder_blocks_in_memory]
        )
        svi_enable_refiner_block_swap.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[svi_enable_refiner_block_swap],
            outputs=[svi_refiner_blocks_in_memory]
        )

        # SVI-LTX Image dimension handlers
        svi_input_image.change(
            fn=update_image_dimensions,
            inputs=[svi_input_image],
            outputs=[svi_original_dims, svi_width, svi_height]
        )

        svi_scale_slider.change(
            fn=update_resolution_from_scale,
            inputs=[svi_scale_slider, svi_original_dims],
            outputs=[svi_width, svi_height]
        )

        svi_calc_width_btn.click(
            fn=calculate_width_from_height,
            inputs=[svi_height, svi_original_dims],
            outputs=[svi_width]
        )

        svi_calc_height_btn.click(
            fn=calculate_height_from_width,
            inputs=[svi_width, svi_original_dims],
            outputs=[svi_height]
        )

        # SVI-LTX Video extension dimension update
        def update_svi_video_dimensions(video_path):
            """Update dimensions when video is uploaded for extension."""
            if not video_path or not os.path.exists(video_path):
                return "", gr.update(), gr.update(), gr.update(), gr.update()
            try:
                import cv2
                cap = cv2.VideoCapture(video_path)
                w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS)
                cap.release()
                # Round to nearest 64
                w = round(w / 64) * 64
                h = round(h / 64) * 64
                fps = max(1, int(round(fps)))
                return f"{w}x{h}", gr.update(value=w), gr.update(value=h), gr.update(), gr.update(value=fps)
            except Exception:
                return "", gr.update(), gr.update(), gr.update(), gr.update()

        svi_extend_video.change(
            fn=update_svi_video_dimensions,
            inputs=[svi_extend_video],
            outputs=[svi_original_dims, svi_width, svi_height, svi_num_frames, svi_frame_rate]
        )

        # =================================================================
        # Save/Load Defaults
        # =================================================================
        lt1_ui_default_components_ORDERED_LIST = [
            # Prompts
            prompt, negative_prompt,
            # Model paths
            checkpoint_path, distilled_checkpoint, stage2_checkpoint, gemma_root,
            spatial_upsampler_path, distilled_lora_path, distilled_lora_strength,
            # Generation parameters
            mode, pipeline, width, height, num_frames, frame_rate,
            cfg_guidance_scale, num_inference_steps, stage2_steps, seed,
            # Image conditioning (not input_image itself - that's a file upload)
            image_frame_idx, image_strength,
            end_image_strength,
            # Anchor conditioning
            anchor_interval, anchor_strength, anchor_decay,
            # Refine settings
            refine_strength, refine_steps,
            # Audio and prompt
            disable_audio, enhance_prompt,
            # Memory optimization
            offload, enable_fp8,
            enable_dit_block_swap, dit_blocks_in_memory,
            enable_text_encoder_block_swap, text_encoder_blocks_in_memory,
            enable_refiner_block_swap, refiner_blocks_in_memory,
            # LoRA
            lora_folder, user_lora, user_lora_strength,
            # Output
            save_path, batch_size,
            # Scale slider
            scale_slider,
        ]

        lt1_ui_default_keys = [
            # Prompts
            "prompt", "negative_prompt",
            # Model paths
            "checkpoint_path", "distilled_checkpoint", "stage2_checkpoint", "gemma_root",
            "spatial_upsampler_path", "distilled_lora_path", "distilled_lora_strength",
            # Generation parameters
            "mode", "pipeline", "width", "height", "num_frames", "frame_rate",
            "cfg_guidance_scale", "num_inference_steps", "stage2_steps", "seed",
            # Image conditioning
            "image_frame_idx", "image_strength",
            "end_image_strength",
            # Anchor conditioning
            "anchor_interval", "anchor_strength", "anchor_decay",
            # Refine settings
            "refine_strength", "refine_steps",
            # Audio and prompt
            "disable_audio", "enhance_prompt",
            # Memory optimization
            "offload", "enable_fp8",
            "enable_dit_block_swap", "dit_blocks_in_memory",
            "enable_text_encoder_block_swap", "text_encoder_blocks_in_memory",
            "enable_refiner_block_swap", "refiner_blocks_in_memory",
            # LoRA
            "lora_folder", "user_lora", "user_lora_strength",
            # Output
            "save_path", "batch_size",
            # Scale slider
            "scale_slider",
        ]

        def save_lt1_defaults(*values):
            os.makedirs(UI_CONFIGS_DIR, exist_ok=True)
            settings_to_save = {}
            for i, key in enumerate(lt1_ui_default_keys):
                settings_to_save[key] = values[i]
            try:
                with open(LT1_DEFAULTS_FILE, 'w') as f:
                    json.dump(settings_to_save, f, indent=2)
                return "LTX defaults saved successfully."
            except Exception as e:
                return f"Error saving LTX defaults: {e}"

        def load_lt1_defaults(request: gr.Request = None):
            lora_folder_val = "lora"
            lora_choices = get_ltx_lora_options(lora_folder_val)

            if not os.path.exists(LT1_DEFAULTS_FILE):
                if request:
                    return [gr.update()] * len(lt1_ui_default_keys) + ["No defaults file found."]
                else:
                    return [gr.update()] * len(lt1_ui_default_keys) + [""]

            try:
                with open(LT1_DEFAULTS_FILE, 'r') as f:
                    loaded_settings = json.load(f)
            except Exception as e:
                return [gr.update()] * len(lt1_ui_default_keys) + [f"Error loading defaults: {e}"]

            # Update lora folder from settings
            lora_folder_val = loaded_settings.get("lora_folder", "lora")
            lora_choices = get_ltx_lora_options(lora_folder_val)

            updates = []
            for i, key in enumerate(lt1_ui_default_keys):
                component = lt1_ui_default_components_ORDERED_LIST[i]
                default_value_from_component = None
                if hasattr(component, 'value'):
                    default_value_from_component = component.value

                value_to_set = loaded_settings.get(key, default_value_from_component)

                # Special handling for LoRA dropdown
                if key == "user_lora":
                    if value_to_set not in lora_choices:
                        value_to_set = "None"
                    updates.append(gr.update(choices=lora_choices, value=value_to_set))
                else:
                    updates.append(gr.update(value=value_to_set))

            return updates + ["LTX defaults loaded successfully."]

        lt1_save_defaults_btn.click(
            fn=save_lt1_defaults,
            inputs=lt1_ui_default_components_ORDERED_LIST,
            outputs=[lt1_defaults_status]
        )
        lt1_load_defaults_btn.click(
            fn=load_lt1_defaults,
            inputs=None,
            outputs=lt1_ui_default_components_ORDERED_LIST + [lt1_defaults_status]
        )

        def initial_load_lt1_defaults():
            results_and_status = load_lt1_defaults(None)
            return results_and_status[:-1]

        demo.load(
            fn=initial_load_lt1_defaults,
            inputs=None,
            outputs=lt1_ui_default_components_ORDERED_LIST
        )

        # =================================================================
        # SVI-LTX Save/Load Defaults
        # =================================================================
        SVI_LTX_DEFAULTS_FILE = os.path.join(UI_CONFIGS_DIR, "svi_ltx_defaults.json")

        svi_ltx_ui_default_components_ORDERED_LIST = [
            # Prompts
            svi_prompt1, svi_prompt2, svi_prompt3, svi_prompt4,
            svi_prompt5, svi_prompt6, svi_prompt7, svi_prompt8,
            svi_negative_prompt,
            # SVI settings
            svi_num_clips, svi_overlap_frames, svi_num_motion_latent,
            svi_num_motion_frame, svi_seed_multiplier,
            # Generation params
            svi_width, svi_height, svi_num_frames, svi_frame_rate,
            svi_inference_steps, svi_cfg_scale, svi_seed,
            # Anchor settings
            svi_anchor_interval, svi_anchor_strength, svi_anchor_decay,
            # Model settings
            svi_checkpoint_path, svi_gemma_root, svi_spatial_upsampler,
            svi_distilled_lora, svi_distilled_lora_strength, svi_one_stage,
            svi_enable_fp8, svi_offload,
            # Block swap
            svi_enable_dit_block_swap, svi_dit_blocks_in_memory,
            svi_enable_text_encoder_block_swap, svi_text_encoder_blocks_in_memory,
            svi_enable_refiner_block_swap, svi_refiner_blocks_in_memory,
            # LoRA
            svi_lora_folder, svi_lora_dropdown, svi_lora_strength,
            # Output
            svi_disable_audio, svi_output_path, svi_batch_size,
            # Video extension
            svi_frames_to_check, svi_prepend_original,
            # Scale slider
            svi_scale_slider,
        ]

        svi_ltx_ui_default_keys = [
            # Prompts
            "svi_prompt1", "svi_prompt2", "svi_prompt3", "svi_prompt4",
            "svi_prompt5", "svi_prompt6", "svi_prompt7", "svi_prompt8",
            "svi_negative_prompt",
            # SVI settings
            "svi_num_clips", "svi_overlap_frames", "svi_num_motion_latent",
            "svi_num_motion_frame", "svi_seed_multiplier",
            # Generation params
            "svi_width", "svi_height", "svi_num_frames", "svi_frame_rate",
            "svi_inference_steps", "svi_cfg_scale", "svi_seed",
            # Anchor settings
            "svi_anchor_interval", "svi_anchor_strength", "svi_anchor_decay",
            # Model settings
            "svi_checkpoint_path", "svi_gemma_root", "svi_spatial_upsampler",
            "svi_distilled_lora", "svi_distilled_lora_strength", "svi_one_stage",
            "svi_enable_fp8", "svi_offload",
            # Block swap
            "svi_enable_dit_block_swap", "svi_dit_blocks_in_memory",
            "svi_enable_text_encoder_block_swap", "svi_text_encoder_blocks_in_memory",
            "svi_enable_refiner_block_swap", "svi_refiner_blocks_in_memory",
            # LoRA
            "svi_lora_folder", "svi_lora_dropdown", "svi_lora_strength",
            # Output
            "svi_disable_audio", "svi_output_path", "svi_batch_size",
            # Video extension
            "svi_frames_to_check", "svi_prepend_original",
            # Scale slider
            "svi_scale_slider",
        ]

        def save_svi_ltx_defaults(*values):
            os.makedirs(UI_CONFIGS_DIR, exist_ok=True)
            settings_to_save = {}
            for i, key in enumerate(svi_ltx_ui_default_keys):
                settings_to_save[key] = values[i]
            try:
                with open(SVI_LTX_DEFAULTS_FILE, 'w') as f:
                    json.dump(settings_to_save, f, indent=2)
                return "SVI-LTX defaults saved successfully."
            except Exception as e:
                return f"Error saving SVI-LTX defaults: {e}"

        def load_svi_ltx_defaults(request: gr.Request = None):
            lora_folder_val = "lora"
            lora_choices = get_ltx_lora_options(lora_folder_val)

            if not os.path.exists(SVI_LTX_DEFAULTS_FILE):
                if request:
                    return [gr.update()] * len(svi_ltx_ui_default_keys) + ["No SVI-LTX defaults file found."]
                else:
                    return [gr.update()] * len(svi_ltx_ui_default_keys) + [""]

            try:
                with open(SVI_LTX_DEFAULTS_FILE, 'r') as f:
                    loaded_settings = json.load(f)
            except Exception as e:
                return [gr.update()] * len(svi_ltx_ui_default_keys) + [f"Error loading SVI-LTX defaults: {e}"]

            # Update lora folder from settings
            lora_folder_val = loaded_settings.get("svi_lora_folder", "lora")
            lora_choices = get_ltx_lora_options(lora_folder_val)

            updates = []
            for i, key in enumerate(svi_ltx_ui_default_keys):
                component = svi_ltx_ui_default_components_ORDERED_LIST[i]
                default_value_from_component = None
                if hasattr(component, 'value'):
                    default_value_from_component = component.value

                value_to_set = loaded_settings.get(key, default_value_from_component)

                # Special handling for LoRA dropdown
                if key == "svi_lora_dropdown":
                    if value_to_set not in lora_choices:
                        value_to_set = "None"
                    updates.append(gr.update(choices=lora_choices, value=value_to_set))
                else:
                    updates.append(gr.update(value=value_to_set))

            return updates + ["SVI-LTX defaults loaded successfully."]

        svi_save_defaults_btn.click(
            fn=save_svi_ltx_defaults,
            inputs=svi_ltx_ui_default_components_ORDERED_LIST,
            outputs=[svi_batch_progress]
        )
        svi_load_defaults_btn.click(
            fn=load_svi_ltx_defaults,
            inputs=None,
            outputs=svi_ltx_ui_default_components_ORDERED_LIST + [svi_batch_progress]
        )

        def initial_load_svi_ltx_defaults():
            results_and_status = load_svi_ltx_defaults(None)
            return results_and_status[:-1]

        demo.load(
            fn=initial_load_svi_ltx_defaults,
            inputs=None,
            outputs=svi_ltx_ui_default_components_ORDERED_LIST
        )

        return demo


# =============================================================================
# Port Detection
# =============================================================================

def find_available_port(start_port: int = 7860, max_attempts: int = 100) -> int:
    """Find the first available port starting from start_port."""
    import socket
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('', port))
                return port
        except OSError:
            continue
    raise RuntimeError(f"Could not find available port in range {start_port}-{start_port + max_attempts}")


# =============================================================================
# Main
# =============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LTX-2 Video Generation UI")
    parser.add_argument("--port", type=int, default=None,
                        help="Server port. Auto-detects available port if not specified.")
    parser.add_argument("--share", action="store_true",
                        help="Create a public Gradio link.")
    args = parser.parse_args()

    # Determine port (auto-detect if not specified)
    if args.port is not None:
        port = args.port
    else:
        port = find_available_port(7860)

    cuda_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "all")
    print(f"[lt1.py] Starting on port {port}, CUDA_VISIBLE_DEVICES={cuda_devices}")

    demo = create_interface()
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=args.share,
        inbrowser=False
    )
